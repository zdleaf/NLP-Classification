{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import string\n",
    "import re\n",
    "import math \n",
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import NuSVC\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from nltk.tag import CRFTagger\n",
    "from nltk import pos_tag\n",
    "from nltk.classify import SklearnClassifier\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('always') # always show warnings, useful for SK Learning warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our n-gram language module code from ngrammodel.py\n",
    "import ngrammodel as ngram\n",
    "\n",
    "# to refresh our module if changed\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL PARAMETERS\n",
    "\n",
    "ngramGender = False # enable perplexity from N-gram models on gender to be added as features\n",
    "ngramCharacter = False # enable character N-gram model features\n",
    "sentenceLength = False # add length of sentence as feature\n",
    "stopsRemoved = False # add number of stop words removed as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureDict = {} # a global dictionary of features\n",
    "trainPath = 'training.csv'\n",
    "testPath = 'test.csv'\n",
    "\n",
    "def parseLine(line):\n",
    "    # line[0] = TEXT/SCRIPT LINE\n",
    "    # line[1] = CHARACTER NAME\n",
    "    # line[2] = GENDER\n",
    "    return (line[0], line[1], line[2])\n",
    "\n",
    "def loadData(path):\n",
    "    data = []\n",
    "    with open(path, encoding=\"latin-1\") as file: \n",
    "        reader = csv.reader(file, delimiter=',')\n",
    "        for line in reader:\n",
    "            # print(line)\n",
    "            (Text, Name, Gender) = parseLine(line)\n",
    "            data.append((Text, Name, Gender))\n",
    "        print(\"All data loaded from \" + path)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data loaded from training.csv\n"
     ]
    }
   ],
   "source": [
    "trainData = loadData(trainPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:23: DeprecationWarning: invalid escape sequence \\.\n",
      "<>:23: DeprecationWarning: invalid escape sequence \\.\n",
      "<>:23: DeprecationWarning: invalid escape sequence \\.\n",
      "<ipython-input-6-fb775e6bfc81>:23: DeprecationWarning: invalid escape sequence \\.\n",
      "  fullStops = re.findall('(?<!\\.)(?!\\.\\.)([.])', text)\n"
     ]
    }
   ],
   "source": [
    "# preProcess - process one sentence and return a list of processed tokens\n",
    "# includes various normalisation options\n",
    "#\n",
    "# parameters:\n",
    "# @punc - include punctuation as features\n",
    "# @stem - simplify into stems using NLTK\n",
    "# @stop - remove stop words using either SKLearn = 1 or NLTK = 2\n",
    "# @lemma - lemmatize with NLTK (only to be used in addition with crftag=3)\n",
    "# @bigrams - include bigrams as features instead of unigrams\n",
    "# @crftag - include POS tags, see crftagging() for details\n",
    "\n",
    "def preProcess(text, punc=1, stem=1, stop=2, lemma=0, bigrams=0, crftag=3):\n",
    "    # save a list of the punctuation marks before removal, so they can be re-added as features\n",
    "    regexPunc = re.compile('[^_\\'&+?Â£:;!,-]')\n",
    "    puncList = regexPunc.sub('', text)\n",
    "    \n",
    "    # count the number of pauses in the sentence\n",
    "    pauses = text.count(\"...\")\n",
    "    \n",
    "    # count the number of full stops that are not pauses ('...')\n",
    "    # regexp matches only '.' that is not preceded by another '.', or followed by another '.'\n",
    "    # (i hate regexp - this took far too long)\n",
    "    fullStops = re.findall('(?<!\\.)(?!\\.\\.)([.])', text)\n",
    "    \n",
    "    # clean up data set by removing unwanted characters\n",
    "    regex = re.compile('[^a-zA-Z0-9\\' ]')\n",
    "    text = regex.sub(' ', text) # replace unmatched characters with whitespace (except apostrophes)\n",
    "    \n",
    "    # now remove apostrophes but don't replace with whitespace\n",
    "    regex2 = re.compile('[^a-zA-Z0-9 ]')\n",
    "    text = regex2.sub('', text)\n",
    "       \n",
    "    # make the text lowercase for consistency\n",
    "    text = text.lower()  \n",
    "    \n",
    "    # split by whitespace\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # save our basic pre-processed tokens to be passed to n-gram language models later\n",
    "    ngramTokens = tokens\n",
    "    \n",
    "    # get a list of our CRF tags to be added later\n",
    "    if (crftag != 0): # from nltk.tag import CRFTagger\n",
    "        crftags = crftagging(tokens, crftag)\n",
    "    \n",
    "    sentLength = len(tokens)\n",
    "    \n",
    "    # stem words using NLTK\n",
    "    if (stem == 1):\n",
    "        tokens = stemTokens(tokens)\n",
    "        \n",
    "    # bigrams\n",
    "    if (bigrams == 1):\n",
    "        new_tokens = []\n",
    "        for i in range(len(tokens)-1):\n",
    "            new_tokens.append(tokens[i] + \" \" + tokens[i+1])\n",
    "        tokens = new_tokens\n",
    "    \n",
    "    # keep a count of tokens before stop removal\n",
    "    tokenCount = len(tokens)\n",
    "    \n",
    "    # remove stop words using sklearn\n",
    "    if (stop == 1):\n",
    "        tokens = stopTokens(tokens, 1)\n",
    "        \n",
    "    # remove stop words using NLTK\n",
    "    if (stop == 2):\n",
    "        tokens = stopTokens(tokens, 2)\n",
    "    \n",
    "    # calculate how many stop words we removed\n",
    "    stopsRemoved = tokenCount - len(tokens)\n",
    "    \n",
    "    # implement lemma with NLTK\n",
    "    if (lemma == 1):\n",
    "        tokens = lemmaTokens(tokens)\n",
    "    \n",
    "    # append our punctuation characters as features\n",
    "    if (punc == 1):\n",
    "        for x in puncList:\n",
    "            tokens.append(x)\n",
    "    \n",
    "        # append our full stops as features (i.e. rough encoding of number of sentences per line)\n",
    "        for y in fullStops:\n",
    "            tokens.append(y)\n",
    "        \n",
    "        # append the correct number of pauses as features\n",
    "        for i in range(pauses):\n",
    "            tokens.append(\"...\")\n",
    "        \n",
    "    # merge our CRF tag list with our token list if required\n",
    "    if (crftag != 0):\n",
    "        tokens += crftags\n",
    "    \n",
    "    return tokens, stopsRemoved, sentLength, ngramTokens\n",
    "    \n",
    "# simplify words into their stems using the SnowballStemmer from NLTK\n",
    "def stemTokens(tokens):\n",
    "    new_tokens = []\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    for token in tokens:\n",
    "        new_tokens.append(stemmer.stem(token))\n",
    "    return new_tokens\n",
    "\n",
    "# remove stop words\n",
    "# lib = 1, use SKLearn\n",
    "# lib = 2, use NLTK\n",
    "def stopTokens(tokens, lib):\n",
    "    # remove using SKLearn\n",
    "    # list of removed words: https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/stop_words.py\n",
    "    if (lib == 1):\n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "        x = tfidf_vectorizer.fit_transform(tokens)\n",
    "        new_tokens = tfidf_vectorizer.get_feature_names()\n",
    "        \n",
    "    # remove stop words using NLTK\n",
    "    # type set(stopwords.words('english')) in interpreter to see list of words\n",
    "    if (lib == 2):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        new_tokens = [w for w in tokens if not w in stop_words] \n",
    "        \n",
    "    return new_tokens\n",
    "\n",
    "# Lemmatization of tokens with NLTK\n",
    "# input is a list of (token,tag) tuples - the output of crftagging() with combined=1 and tagger=3 arguments\n",
    "def lemmaTokens(tokens):\n",
    "    taggedTokens = crftagging(tokens, tagger=3, combined=1)\n",
    "    new_tokens = []\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    for pair in taggedTokens:\n",
    "        # crftagging(tagger=3) returns 12 universal tags - match these to wordnet POS types, otherwise use default NOUN\n",
    "        wordnetPOS = [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]\n",
    "        if pair[1] == \"NOUN\":\n",
    "            tag = wordnet.NOUN\n",
    "        if pair[1] == \"VERB\":\n",
    "            tag = wordnet.VERB\n",
    "        if pair[1] == \"ADJ\":\n",
    "            tag = wordnet.ADJ\n",
    "        if pair[1] == \"ADV\":\n",
    "            tag = wordnet.ADV\n",
    "        else:\n",
    "            tag = wordnet.NOUN\n",
    "        new_tokens.append(lemmatizer.lemmatize(pair[0], tag)) \n",
    "    return new_tokens\n",
    "\n",
    "# returns a list of the POS tags, using one of 3 tagging methods\n",
    "# parameters:\n",
    "# @tokens - list of words to be tagged\n",
    "# @tagger - tagging method as below\n",
    "# @combined - whether to return a list of [(token,tag)] tuples, to pass to lemmaTokens() function\n",
    "def crftagging(tokens, tagger=3, combined=0):\n",
    "    crftags = []\n",
    "    if tagger == 1:\n",
    "        TAGGER_PATH = \"trainedCRFtagger\"   # pre-trained POS-tagger\n",
    "        tagger = CRFTagger()\n",
    "        tagger.set_model_file(TAGGER_PATH)\n",
    "        taggedTokens = tagger.tag(tokens)\n",
    "    if tagger == 2:\n",
    "        taggedTokens = pos_tag(tokens)\n",
    "    # output universal tagset of Petrov, Das, & McDonald\n",
    "    if tagger == 3:\n",
    "        taggedTokens = pos_tag(tokens, tagset='universal')\n",
    "    \n",
    "    if(combined == 1):\n",
    "        return taggedTokens\n",
    "    else:\n",
    "        for pair in taggedTokens:\n",
    "            tag = pair[1]\n",
    "            crftags.append(tag)\n",
    "        return crftags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toFeatureVector - return a weighted dictionary containing text features as keys and weights as values\n",
    "# takes the tuple returned by preProcess() as input\n",
    "#\n",
    "# preProcessed[0] - tokens/features to be counted\n",
    "# preProcessed[1] - the amount of stop words removed\n",
    "# preProcessed[2] - total sentence length\n",
    "# preProcessed[3] - list of basic preProcessed words for our language models\n",
    "\n",
    "def toFeatureVector(preProcessed):\n",
    "    featureVector = {}\n",
    "    for token in preProcessed[0]:\n",
    "        \n",
    "        # feature dictionary\n",
    "        if token not in featureVector:\n",
    "            featureVector[token] = 1\n",
    "        else:\n",
    "            featureVector[token] += 1\n",
    "            \n",
    "        # global dictionary\n",
    "        if token not in featureDict:\n",
    "            featureDict[token] = 1\n",
    "        else:\n",
    "            featureDict[token] += 1\n",
    "            \n",
    "    if(stopsRemoved == True):      \n",
    "        featureVector[\"stopsRemoved\"] = preProcessed[1]\n",
    "    if(sentenceLength == True):   \n",
    "        featureVector[\"sentenceLength\"] = preProcessed[2]\n",
    "    \n",
    "    return featureVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preProcess and add featureVectors for all data\n",
    "def processVector(rawData):\n",
    "    processedData = []\n",
    "    for (Text, Name, Gender) in rawData:\n",
    "        preProcessed = preProcess(Text)\n",
    "        ngramText = preProcessed[3]\n",
    "        processedData.append((toFeatureVector(preProcessed),Name,Gender,ngramText))\n",
    "    return processedData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training N-gram language models with Kneser-Ney smoothing\n",
    "\n",
    "Code below is for training ngram language models using Kneser-Ney (KN) smoothing. Majority of the code for this is in ./ngrammodel.py and imported into this notebook.\n",
    "\n",
    "The idea is to split the data by label, e.g. male/female, and train an ngram model on each. For each sentence, the perplexity of that sentence when returned by each language model was added as a feature. In theory this will allow us to encode typical speech patterns, phrases, words of either male/female speaker, or for each character. If a particular sentence has a lower perplexity in one language model, this means it matches this model closer than the others.\n",
    "\n",
    "The KN smoothing takes two parameters, order and discount. In order to tune for these parameters below, a simple training/heldout split is used to test different values.\n",
    "\n",
    "Since perplexity has to added as a feature to every sentence, a cross-validation method is used in the function to do this (addNGramFeatures). If we include the same sentence we are trying to get perplexity of in the training of the language model, we will get a very low perplexity as this sentence has been \"seen\" before. Therefore to tag the entire training data with perplexities, and also ensure our language models are sufficiently trained, a 5 fold training cross-validation method is implemented, where we train 4/5 of the data and tag the remaining 1/5, over 5 iterations to tag the entire set. This is preferable over say splitting the data 50% and tagging the remaining 50% and vice versa, as we are training the language model on more data each time.\n",
    "\n",
    "This allows us to have accurate accurate perplexity features when we're running cross-validation on the classifier. For the final classification on the test data, the language models are trained on the entire training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get basic pre-processed text (i.e. lowercase, removed punctuation and encoding errors) \n",
    "# from the training data to train our ngram language models \n",
    "def preProcessNGram(rawData):\n",
    "    ngramData = []\n",
    "    for (Text, Name, Gender) in rawData:\n",
    "        ngramData.append((preProcess(Text, punc=0, stem=0, stop=0, lemma=0, bigrams=0, crftag=0),Name,Gender))\n",
    "    return ngramData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processNGram(ngramData):\n",
    "    # create a dictionary of unigram counts\n",
    "    unigrams = ngram.countUnigrams(ngramData)\n",
    "    # replace any words only seen once with </unk>\n",
    "    vocab = ngram.minDocFrequency(unigrams, 2)\n",
    "    # train on the data/vocab and return\n",
    "    trainedKN = ngram.trainKN(ngramData, vocab, order)\n",
    "    # returns a tuple of the trained KN and the vocab\n",
    "    return (trainedKN, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentencePerplexity(sentences, tupleKN):\n",
    "    # tupleKN[0] = trainedKN\n",
    "    # tupleKN[1] = vocab\n",
    "    return math.trunc(ngram.perplexityKN(sentences, tupleKN[1], order, discount, False, tupleKN[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramData = preProcessNGram(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the order and discount parameters to use for the Kneser-Ney smoothing\n",
    "order = 5\n",
    "discount = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic 80/20 training/holdout split for tuning KN parameters\n",
    "maleData = []\n",
    "femaleData = []\n",
    "# build a list of male/female only sentences to train our ngram language model\n",
    "for x in ngramData:\n",
    "    if x[2] == 'male':\n",
    "        maleData.append(x[0][0])\n",
    "    if x[2] == 'female':\n",
    "        femaleData.append(x[0][0])\n",
    "\n",
    "# split into training and heldout\n",
    "maleTrain = maleData[0:int(len(maleData)*0.8)]\n",
    "maleHeldout = maleData[int(len(maleData)*0.8):]\n",
    "\n",
    "femaleTrain = femaleData[0:int(len(femaleData)*0.8)]\n",
    "femaleHeldout = femaleData[int(len(femaleData)*0.8):]\n",
    "\n",
    "maleKN = processNGram(maleTrain)\n",
    "femaleKN = processNGram(femaleTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male:  115\n",
      "female:  115\n",
      "order:  3  discount:  0.7 \n",
      "\n",
      "male:  110\n",
      "female:  110\n",
      "order:  3  discount:  0.8 \n",
      "\n",
      "male:  109\n",
      "female:  109\n",
      "order:  3  discount:  0.9 \n",
      "\n",
      "male:  121\n",
      "female:  120\n",
      "order:  4  discount:  0.7 \n",
      "\n",
      "male:  112\n",
      "female:  112\n",
      "order:  4  discount:  0.8 \n",
      "\n",
      "male:  109\n",
      "female:  109\n",
      "order:  4  discount:  0.9 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing different order and discount values for the male/female models\n",
    "# discountList = [0.2, 0.4, 0.6, 0.8, 0.9, 0.925, 0.95] \n",
    "discountList = [0.7, 0.8, 0.9] # discounts to attempt for each order\n",
    "\n",
    "for i in range(3,5): # orders 3,4\n",
    "    order = i\n",
    "    for j in discountList:\n",
    "        discount = j\n",
    "        print(\"male: \", getSentencePerplexity(maleHeldout, maleKN))\n",
    "        print(\"female: \", getSentencePerplexity(femaleHeldout, femaleKN))\n",
    "        print(\"order: \", i, \" discount: \", j, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KN tuning results\n",
    "\n",
    "As can be seen below the optimal values for order and discount was found to be 5 and 0.9 respectively.\n",
    "\n",
    "| 1      | 2       | 3        | 4        | 5        | 6       |\n",
    "|--------|---------|----------|----------|----------|---------|\n",
    "| 0\\.2   | 228\\.65 | 134\\.19  | 206\\.77  | 273\\.66  | 321\\.98 | 368\\.54  |\n",
    "| 0\\.4   | 228\\.65 | 115\\.31  | 137\\.18  | 158\\.56  | 173\\.44 | 187\\.14  |\n",
    "| 0\\.6   | 228\\.65 | 107\\.72  | 111\\.43  | 119\\.20  | 124\\.89 | 130\\.11  |\n",
    "| 0\\.8   | 228\\.65 | 105\\.46  | 100\\.46  | 102\\.11  | 103\\.81 | 105\\.45  |\n",
    "| 0\\.9   | 228\\.65 | 106\\.61  | 99\\.44   | **99\\.33**   | 99\\.84  | 100\\.41  |\n",
    "| 0\\.925 | 228\\.65 | 107\\.335 | 99\\.925  | 99\\.505  | 99\\.785 | 100\\.125 |\n",
    "| 0\\.95  | 228\\.65 | 108\\.39  | 100\\.955 | 100\\.295 | 100\\.37 | 100\\.515 |\n",
    "| 0\\.975 | 228\\.65 | 110\\.015 | 102\\.95  | 102\\.195 | 102\\.12 | 102\\.095 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'oscar': 1, 'asleep': 1, \"'\": 1, '.': 1, 'NOUN': 1, 'VERB': 1},\n",
       " 'MAX',\n",
       " 'male',\n",
       " ['oscars', 'asleep'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process all the training data in according with preProcess and feature vector parameters\n",
    "processedTrainData = processVector(trainData)\n",
    "processedTrainData[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the order and discount parameters to use for the Kneser-Ney smoothing\n",
    "order = 5\n",
    "discount = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "addNGramFeatures()\n",
    "\n",
    "We use this function to add the ngram features to all the training data for every sentence\n",
    "(using the 4/5ths cross-validation method as explained above). This function also adds the \n",
    "features to test data when we run the final classification task.\n",
    "\n",
    "If testing=False (default), we run the separate cross-validation method by training on 4/5ths of the data \n",
    "and add the features only to the trainingData.\n",
    "\n",
    "The testing=True flag denotes we should train the ngram model on all the trainData and add perplexity features \n",
    "to the testData.\n",
    "'''\n",
    "def addNGramFeatures(trainData, folds, testData=None, testing=False):\n",
    "    newData = []\n",
    "    \n",
    "    ##### If we're tagging the training data only\n",
    "    if testing == False:\n",
    "        \n",
    "        # setup our cross validation folds\n",
    "        foldSize = int(len(trainData)/folds)\n",
    "        # allow for datasets that do not divide exactly by number of folds - crossTrain data will always include the remainder\n",
    "        remainder = len(trainData) % folds \n",
    "        \n",
    "        for i in range(0,len(trainData)-remainder,foldSize):\n",
    "            crossTest = trainData[i:i+foldSize] # from start of our heldout up to the fold size\n",
    "            crossTrain = trainData[:i] + trainData[i+foldSize:] # from start to start of test, and also from the end of heldout up to end of set\n",
    "            \n",
    "            ##### add the ngram gender tags if specified by global var\n",
    "            if ngramGender == True:\n",
    "                maleData = []\n",
    "                femaleData = []\n",
    "                # train our language models only on the crossTrain data\n",
    "                for line in crossTrain:\n",
    "                    if line[2] == 'male':\n",
    "                        # append the ngram training sentence at index 3 of our dataset\n",
    "                        maleData.append(line[3])\n",
    "                    if line[2] == 'female':\n",
    "                        femaleData.append(line[3])\n",
    "\n",
    "                maleKN = processNGram(maleData)\n",
    "                femaleKN = processNGram(femaleData)\n",
    "                    \n",
    "            if ngramCharacter == True:    \n",
    "                \n",
    "                charDict = {}\n",
    "                charList = [\"SEAN\", \"SHIRLEY\", \"MAX\", \"IAN\", \"MINTY\", \"RONNIE\", \"STACEY\", \"JANE\", \"PHIL\", \"CLARE\", \"TANYA\", \"HEATHER\", \"GARRY\", \"BRADLEY\", \"CHRISTIAN\", \"STEVEN\", \"ROXY\", \"JACK\"]\n",
    "                # initialise a dict with characters as keys and a [[list], None] as a value\n",
    "                # the [list] will store all lines attributed to the character\n",
    "                # 'None' will latter be used to store the trainedKN tuple to pass to getSentencePerplexity\n",
    "                charDict = {k: [[],None] for k in charList}\n",
    "                \n",
    "                # train our language models only on the crossTrain data\n",
    "                for _, name, _, ngramline in crossTrain:\n",
    "                    # append the line to the list at index [0] of our other list\n",
    "                    charDict[name][0].append(ngramline)\n",
    "                # process all the ngrams on the list of text for that character and put it in the index[1] of the charDict list\n",
    "                for char in charDict:\n",
    "                    charDict[char][1] = processNGram(charDict[char][0])\n",
    "\n",
    "            # for each featureVector in crossTest, get perplexity of the sentence and add as a feature\n",
    "            for line in crossTest:\n",
    "                # copy the featureVector into a new one (so we don't ammend the old one)\n",
    "                featureVector = line[0].copy()\n",
    "                ngramLine = line[3]\n",
    "                \n",
    "                if ngramGender == True:\n",
    "                    featureVector[\"male\"] = getSentencePerplexity([ngramLine], maleKN)\n",
    "                    featureVector[\"female\"] = getSentencePerplexity([ngramLine], femaleKN)\n",
    "                    \n",
    "                if ngramCharacter == True:\n",
    "                    for char in charDict:\n",
    "                        featureVector[char] = getSentencePerplexity([ngramLine], charDict[char][1])\n",
    "                    \n",
    "                new_line = (featureVector, line[1], line[2], line[3])\n",
    "                newData.append(new_line)\n",
    "                \n",
    "            continue\n",
    "            \n",
    "        return newData\n",
    "            \n",
    "    ##### If we're on final testing, train on all the trainData and tag the testData        \n",
    "    if testing == True:\n",
    "        if ngramGender == True:\n",
    "            maleData = []\n",
    "            femaleData = []\n",
    "\n",
    "            for line in trainData:\n",
    "                if line[2] == 'male':\n",
    "                    maleData.append(line[3])\n",
    "                if line[2] == 'female':\n",
    "                    femaleData.append(line[3])\n",
    "\n",
    "            maleKN = processNGram(maleData)\n",
    "            femaleKN = processNGram(femaleData)\n",
    "        \n",
    "        if ngramCharacter == True:\n",
    "            \n",
    "            charDict = {}\n",
    "            charList = [\"SEAN\", \"SHIRLEY\", \"MAX\", \"IAN\", \"MINTY\", \"RONNIE\", \"STACEY\", \"JANE\", \"PHIL\", \"CLARE\", \"TANYA\", \"HEATHER\", \"GARRY\", \"BRADLEY\", \"CHRISTIAN\", \"STEVEN\", \"ROXY\", \"JACK\"]\n",
    "            charDict = {k: [[],None] for k in charList}\n",
    "            \n",
    "            # train our language models on the entire training data\n",
    "            for _, name, _, ngramline in trainData:\n",
    "                # append the line to the list at index [0] of our other list\n",
    "                charDict[name][0].append(ngramline)\n",
    "\n",
    "            # process all the ngrams on the list of text for that character and put it in the index[1] of the charDict list\n",
    "            for char in charDict:\n",
    "                charDict[char][1] = processNGram(charDict[char][0])\n",
    "        \n",
    "        # for each featureVector in testData, get perplexity of the sentence and add as a feature\n",
    "        for line in testData:\n",
    "            featureVector = line[0].copy()\n",
    "            ngramLine = line[3]\n",
    "\n",
    "            if ngramGender == True:\n",
    "                featureVector[\"male\"] = getSentencePerplexity([ngramLine], maleKN)\n",
    "                featureVector[\"female\"] = getSentencePerplexity([ngramLine], femaleKN)\n",
    "\n",
    "            if ngramCharacter == True:\n",
    "                for char in charDict:\n",
    "                    featureVector[char] = getSentencePerplexity([ngramLine], charDict[char][1]) \n",
    "                    \n",
    "            new_line = (featureVector, line[1], line[2], line[3])\n",
    "            newData.append(new_line)\n",
    "            \n",
    "        return newData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add gender ngram perplexities if required by the global vars\n",
    "if(ngramGender == True or ngramCharacter == True):\n",
    "    finalTrainData = addNGramFeatures(processedTrainData, 5)\n",
    "else:\n",
    "    finalTrainData = processedTrainData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of classifier\n",
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    #return SklearnClassifier(LinearSVC(C=0.05, max_iter=10000, loss='squared_hinge', class_weight='balanced')).train(trainData)\n",
    "    #return SklearnClassifier(NuSVC(cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf', max_iter=-1, nu=0.5, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False)).train(trainData)\n",
    "    #return SklearnClassifier(BernoulliNB()).train(trainData)\n",
    "    #return SklearnClassifier(MultinomialNB(alpha=1.0, fit_prior=False, class_prior=None)).train(trainData)\n",
    "    return SklearnClassifier(ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)).train(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidate(dataset, folds):\n",
    "    shuffle(dataset) # shuffle only once before implementing k-folds\n",
    "    cv_results = []\n",
    "    sk_reports = []\n",
    "    foldSize = int(len(dataset)/folds)\n",
    "    # allow for datasets that do not divide exactly by number of folds - crossTrain data will always include the remainder\n",
    "    remainder = len(dataset) % folds \n",
    "    \n",
    "    for i in range(0,len(dataset)-remainder,foldSize):\n",
    "        crossTest = dataset[i:i+foldSize] # from start of our heldout up to the fold size\n",
    "        crossTrain = dataset[:i] + dataset[i+foldSize:] # from start to start of test, and also from the end of heldout up to end of set\n",
    "        #print(\"test: \", len(crossTest))\n",
    "        #print(\"train: \", len(crossTrain))\n",
    "        \n",
    "        # train classifier on the trainData\n",
    "        classifier = trainClassifier(crossTrain)\n",
    "        \n",
    "        # predict on the testData with the classifier\n",
    "        crossPredictions = predictLabels(crossTest, classifier)\n",
    "        #print(\"predictions: \", len(crossPredictions))\n",
    "        \n",
    "        # return the actual labels for the testData\n",
    "        crossActual = [x[1] for x in crossTest] # list comprehension to take only gender/character label from testData\n",
    "        #print(\"actual: \", len(crossActual))\n",
    "        #print(\"---------\")\n",
    "        \n",
    "        # use sklearn.metrics to see how our predictions did\n",
    "        # provide a weighted score to account for imbalance between labels\n",
    "        result = precision_recall_fscore_support(crossActual, crossPredictions, average='weighted')\n",
    "\n",
    "        cv_results.append(result)\n",
    "\n",
    "        continue\n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our current data includes both gender and character labels - remove the character/gender labels\n",
    "def splitGender(processedData):\n",
    "    genderData = []\n",
    "    for (Text, Name, Gender, _) in processedData:\n",
    "        genderData.append((Text,Gender)) \n",
    "    return genderData\n",
    "\n",
    "def splitCharacter(processedData):\n",
    "    characterData = []\n",
    "    for (Text, Name, Gender, _) in processedData:\n",
    "        characterData.append((Text,Name)) \n",
    "    return characterData\n",
    "\n",
    "genderTrain = splitGender(finalTrainData)\n",
    "characterTrain = splitCharacter(finalTrainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictLabels(reviewSamples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: t[0], reviewSamples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'wed': 1, 'well': 1, 'caus': 1, 'shirley': 3, 'go': 1, 'bridesmaid': 1, \"'\": 1, ',': 1, '!': 1, '...': 2, 'PRON': 4, 'ADV': 1, 'DET': 1, 'NOUN': 6, 'VERB': 4, 'ADJ': 1, 'PRT': 1, 'CONJ': 1}, 'HEATHER', 'female')\n",
      "Number of features:  4172\n"
     ]
    }
   ],
   "source": [
    "# An example of the features before running the classifier\n",
    "print(finalTrainData[115][0:3])\n",
    "print(\"Number of features: \", len(featureDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Average:  [0.5858970714303553, 0.585459940652819, 0.5854145001898793]\n"
     ]
    }
   ],
   "source": [
    "cv_results = crossValidate(genderTrain, 10)\n",
    "\n",
    "total = [0,0,0]\n",
    "for x in cv_results:\n",
    "    total[0] += x[0] # precision\n",
    "    total[1] += x[1] # recall\n",
    "    total[2] += x[2] # f score\n",
    "\n",
    "# averages\n",
    "total[0] = total[0]/len(cv_results)\n",
    "total[1] = total[1]/len(cv_results)\n",
    "total[2] = total[2]/len(cv_results)\n",
    "\n",
    "print(\"Average: \", total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Average:  [0.2150325481175268, 0.21899109792284865, 0.20906719146529662]\n"
     ]
    }
   ],
   "source": [
    "cv_results = crossValidate(characterTrain, 10)\n",
    "\n",
    "total = [0,0,0]\n",
    "for x in cv_results:\n",
    "    total[0] += x[0] # precision\n",
    "    total[1] += x[1] # recall\n",
    "    total[2] += x[2] # f score\n",
    "\n",
    "# averages\n",
    "total[0] = total[0]/len(cv_results)\n",
    "total[1] = total[1]/len(cv_results)\n",
    "total[2] = total[2]/len(cv_results)\n",
    "\n",
    "print(\"Average: \", total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final classification on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the classifier on trainData and predict on testData\n",
    "def testPrediction(testData, trainData):\n",
    "    # train on the trainData\n",
    "    classifier = trainClassifier(trainData)\n",
    "    # predict on the testData with the classifier\n",
    "    predictions = predictLabels(testData, classifier)\n",
    "    #print(\"predictions: \", len(crossPredictions))\n",
    "    # return the actual labels for the testData\n",
    "    actual = [x[1] for x in testData] # list comprehension to take only gender/character label from testData\n",
    "    # use sklearn.metrics to see how our predictions did\n",
    "    # provide a weighted score to account for imbalance between labels\n",
    "    report = classification_report(actual, predictions, labels=None, target_names=None, sample_weight=None, digits=2, output_dict=False)\n",
    "    result = precision_recall_fscore_support(actual, predictions, average='weighted')\n",
    "    return result, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data loaded from test.csv\n"
     ]
    }
   ],
   "source": [
    "# load the testData\n",
    "testData = loadData(testPath)\n",
    "\n",
    "# process the testData\n",
    "processedTest = processVector(testData)\n",
    "\n",
    "# add ngrams to testData based on language model trained on entire training data if required\n",
    "if(ngramGender == True or ngramCharacter == True):\n",
    "    finalTest = addNGramFeatures(finalTrainData, 5, testData=processedTest, testing=True)\n",
    "else:\n",
    "    finalTest = processedTest\n",
    "\n",
    "# split into genderData and characterData\n",
    "genderTest = splitGender(finalTest)\n",
    "characterTest = splitCharacter(finalTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n",
      "Training Classifier...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.55      0.59      0.57       526\n",
      "        male       0.61      0.58      0.60       598\n",
      "\n",
      "    accuracy                           0.58      1124\n",
      "   macro avg       0.58      0.58      0.58      1124\n",
      "weighted avg       0.58      0.58      0.58      1124\n",
      "\n",
      "(0.5837706014487268, 0.5818505338078291, 0.5822786465035372, None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     BRADLEY       0.20      0.15      0.17        41\n",
      "   CHRISTIAN       0.27      0.17      0.21        46\n",
      "       CLARE       0.13      0.23      0.17        31\n",
      "       GARRY       0.13      0.04      0.06        48\n",
      "     HEATHER       0.23      0.33      0.27        42\n",
      "         IAN       0.23      0.29      0.25       101\n",
      "        JACK       0.25      0.18      0.21        85\n",
      "        JANE       0.38      0.26      0.31        76\n",
      "         MAX       0.19      0.29      0.23        73\n",
      "       MINTY       0.27      0.16      0.20        51\n",
      "        PHIL       0.17      0.13      0.15        53\n",
      "      RONNIE       0.26      0.21      0.23        52\n",
      "        ROXY       0.18      0.09      0.12        56\n",
      "        SEAN       0.09      0.08      0.08        63\n",
      "     SHIRLEY       0.28      0.18      0.22        73\n",
      "      STACEY       0.21      0.17      0.19        72\n",
      "      STEVEN       0.15      0.11      0.13        37\n",
      "       TANYA       0.25      0.52      0.33       124\n",
      "\n",
      "    accuracy                           0.22      1124\n",
      "   macro avg       0.22      0.20      0.20      1124\n",
      "weighted avg       0.22      0.22      0.21      1124\n",
      "\n",
      "(0.22404249054911346, 0.22330960854092527, 0.21133990847549647, None)\n"
     ]
    }
   ],
   "source": [
    "genderResult, genderReport = testPrediction(genderTest, genderTrain)\n",
    "\n",
    "charResult, charReport = testPrediction(characterTest, characterTrain)\n",
    "\n",
    "print(genderReport)\n",
    "print(genderResult)\n",
    "print(charReport)\n",
    "print(charResult)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline performance and general data evalutation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate our baseline performance with random data\n",
    "def calculateBaseline(testData, trainData, predList):\n",
    "    # predict on the testData randomly\n",
    "    predictions = []\n",
    "    for i in range(len(testData)):\n",
    "        predictions.append(random.choice(predList))\n",
    "    # return the actual labels for the testData\n",
    "    actual = [x[1] for x in testData] # list comprehension to take only gender/character label from testData\n",
    "    # use sklearn.metrics to see how our predictions did\n",
    "    # provide a weighted score to account for imbalance between labels\n",
    "    report = classification_report(actual, predictions, labels=None, target_names=None, sample_weight=None, digits=2, output_dict=False)\n",
    "    result = precision_recall_fscore_support(actual, predictions, average='weighted')\n",
    "    return result, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.47      0.52      0.50       526\n",
      "        male       0.54      0.49      0.51       598\n",
      "\n",
      "    accuracy                           0.50      1124\n",
      "   macro avg       0.51      0.51      0.50      1124\n",
      "weighted avg       0.51      0.50      0.50      1124\n",
      "\n",
      "(0.5077182619991688, 0.5044483985765125, 0.5047838268976055, None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     BRADLEY       0.02      0.02      0.02        41\n",
      "   CHRISTIAN       0.07      0.09      0.08        46\n",
      "       CLARE       0.02      0.03      0.02        31\n",
      "       GARRY       0.07      0.08      0.07        48\n",
      "     HEATHER       0.03      0.05      0.04        42\n",
      "         IAN       0.05      0.03      0.04       101\n",
      "        JACK       0.08      0.06      0.07        85\n",
      "        JANE       0.05      0.04      0.04        76\n",
      "         MAX       0.05      0.04      0.04        73\n",
      "       MINTY       0.00      0.00      0.00        51\n",
      "        PHIL       0.04      0.04      0.04        53\n",
      "      RONNIE       0.03      0.04      0.03        52\n",
      "        ROXY       0.04      0.05      0.05        56\n",
      "        SEAN       0.02      0.02      0.02        63\n",
      "     SHIRLEY       0.02      0.01      0.02        73\n",
      "      STACEY       0.08      0.07      0.08        72\n",
      "      STEVEN       0.00      0.00      0.00        37\n",
      "       TANYA       0.16      0.10      0.12       124\n",
      "\n",
      "    accuracy                           0.05      1124\n",
      "   macro avg       0.04      0.04      0.04      1124\n",
      "weighted avg       0.05      0.05      0.05      1124\n",
      "\n",
      "(0.05457562915711595, 0.046263345195729534, 0.0488087531990418, None)\n"
     ]
    }
   ],
   "source": [
    "charList = [\"SEAN\", \"SHIRLEY\", \"MAX\", \"IAN\", \"MINTY\", \"RONNIE\", \"STACEY\", \"JANE\", \"PHIL\", \"CLARE\", \"TANYA\", \"HEATHER\", \"GARRY\", \"BRADLEY\", \"CHRISTIAN\", \"STEVEN\", \"ROXY\", \"JACK\"]\n",
    "genderList = [\"male\", \"female\"]\n",
    "\n",
    "gblResult, gblReport = calculateBaseline(genderTest, genderTrain, genderList)\n",
    "\n",
    "cblResult, cblReport = calculateBaseline(characterTest, characterTrain, charList)\n",
    "\n",
    "print(gblReport)\n",
    "print(gblResult)\n",
    "print(cblReport)\n",
    "print(cblResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique characters:  18\n",
      "{'STACEY': 619, 'PHIL': 500, 'CHRISTIAN': 397, 'IAN': 935, 'TANYA': 1276, 'ROXY': 411, 'JANE': 675, 'BRADLEY': 332, 'MINTY': 454, 'MAX': 714, 'HEATHER': 469, 'STEVEN': 298, 'SHIRLEY': 620, 'GARRY': 340, 'JACK': 591, 'CLARE': 416, 'SEAN': 520, 'RONNIE': 546}\n"
     ]
    }
   ],
   "source": [
    "# character and line count so we can assess our performance\n",
    "charDict = {}\n",
    "for _, char in characterTrain:\n",
    "    if char not in charDict:\n",
    "        charDict[char] = 1\n",
    "    else:\n",
    "        charDict[char] += 1\n",
    "\n",
    "print(\"number of unique characters: \", len(charDict))\n",
    "print(charDict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
